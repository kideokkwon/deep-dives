{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwzEbgyoxXtjFLaafkVTiF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Delta Method\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The Delta method, often used in AB Testing, is a general method for approximating the standard error of the Average Treatment Effect (ATE) estimator when the estimand (the metric) is defined in a way that violates traditional Central Limit theorem rules, which is what is typically used for performing inference (deriving the standard error of the estimate based on Asymptotic normality).\n",
        "\n",
        "For example, when your metric is a percent change (e.g., $\\frac{\\bar Y_T - \\bar Y_c}{\\bar Y_c}$), where $\\bar Y_T$ is the Mean in Treatment and $\\bar Y_C$ is the Mean in Control, the Central Limit does not apply because the CLT, traditionally, only applies for sums (linear functions) of iid random variables, not ratios.\n",
        "\n",
        "However, there are several popular metrics where the metric-of-interest is a ratio, or other types that do not typically obey CLT. Experimentation Platforms, including with **StatSig**, rely on the *Delta Method* for these scenarios, which extends the Central Limit Theorem to allow inference with Ratio and other tricky metrics in a way that is scalable and easy.\n",
        "\n",
        "This notebook is intended to be *self-contained*, so we will review not only the Delta Method but relevant concepts part of understanding the theory behind the Delta Method."
      ],
      "metadata": {
        "id": "zMRG0B2hrPoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delta Method\n",
        "\n",
        "*Taken from [Wikipedia](https://en.wikipedia.org/wiki/Delta_method), adapted to be much more \"beginner-friendly\".*\n",
        "\n",
        "The Delta method says that, if there is a sequence of random variables $X_n$ satisfying\n",
        "\n",
        "$$\\sqrt{n}\\left[X_n-\\theta\\right]\\xrightarrow{D}\\mathcal{N}(0,\\sigma^2),$$\n",
        "\n",
        "where $\\theta,\\sigma^2$ are finite valued constants and $\\xrightarrow{D}$ denotes convergence in distribution, then\n",
        "\n",
        "$$\\sqrt{n}\\left[g(X_n)-g(\\theta)\\right]\\xrightarrow{D}\\mathcal{N}(0,\\sigma^2\\cdot\\left[g'(\\theta)\\right]^2),$$\n",
        "\n",
        "for any function $g$ satisfying the property that its first derivative, evaluated at $\\theta$, $g'(\\theta)$ exists and is non-zero valued.\n",
        "\n",
        "Let's discuss in a bit more detail. The Delta Method says that if the first condition is satisfied, then the second condition satifies too. The first condition says that sequence $X_n$ converges to normality. A \"sequence\" $X_n$ is like a \"list of random variables\", and can be something like a \"sample mean of $n$ coin tosses\", so a random variable that is represented by $n$ , a process that evolves as $n$ increases. In practice, we have a clear example of this. For example, the Delta method is used for Percent Change metrics, where the metric is defined as $\\frac{\\bar Y_T - \\bar Y_C}{\\bar Y_C}$. Each of those components (numerator and denominator) is individually normal because they obey CLT. As you will see later, in the case of multiple random variables, we use a \"multivariate Delta method\", but it is quite analogous to the univarate variant. In this case, the $g$ can be thought of as the nonlinear transformation of the 2 variables that make it a ratio.\n",
        "\n",
        "The intuition of the delta method is that any such $g$ function, in a \"small enough\" range of the function, can be approximated via a first order Taylor series (which is basically a linear function). If the random variable is roughly normal then a linear transformation of it is also normal.\n",
        "\n",
        "In other words, by using a First-Order Taylor series, which turns $g(X_n)$ into a linear function (locally), we can then invoke the CLT again, with the 2nd formula being the result of First-Order Taylor Series. This is because one property of Normal random variables is that a Linear Transformation of a Normal random variable is also a Normal random variable.\n",
        "\n",
        "The text also states that small range can be achieved when approximating the function around the mean, when the variance is \"small enough\". When $g$ is applied to a random variable such as the mean, the delta method would tend to work better as the sample size increases, since it would help reduce the variance, and thus the Taylor approximation would be applied to a smaller range of the function $g$ at the point of interest.\n",
        "\n",
        "This means that for the random variable $X_n$, if we were to use a typical mean as we do in AB tests, then because it tends to be centered around the true value $\\theta$, and that it becomes more precise as sample size grows, the fact that the Taylor series is linear \"locally\" around the true value is justified in its use.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I3ExsRKzrPkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Proof in the Univariate Case\n",
        "\n",
        "*Taken from [Wikipedia](https://en.wikipedia.org/wiki/Delta_method), adapted to be much more \"beginner-friendly\".*\n",
        "\n",
        "Demonstration of this result is straightforward under the assumption that $g(x)$ is differentiable near the neighborhood of $\\theta$ and $g'(x)$ is continuous at $\\theta$ with $g'(\\theta)\\neq 0$.\n",
        "\n",
        "First, we use the First-Order Approximation of a Taylor Series:\n",
        "\n",
        "$$g(X_n)=g(\\theta)+g'(\\tilde\\theta)(X_n-\\theta)$$\n",
        "\n",
        "where $\\tilde\\theta$ lies between $X_n$ and $\\theta$. This comes from the \"Mean Value Theorem\", which is a form of Taylor's theorem and is valid under the assumption that $g$ is differentiable in a neighborhood of $\\theta$.\n",
        "\n",
        "Now, because [convergence in distribution to a constant implies convergence in probability to that constant](https://math.stackexchange.com/questions/1716298/proof-for-convergence-in-distribution-implying-convergence-in-probability-for-co), $X_n\\xrightarrow{P}\\theta$. Also, $|\\tilde\\theta-\\theta|<|X_n-\\theta|$, it must be that $\\tilde\\theta\\xrightarrow{P}\\theta$ and since $g'(\\theta)$ is continuous, the *continuous mapping theorem* yields\n",
        "\n",
        "$$g'(\\tilde\\theta)\\xrightarrow{P}g'(\\theta)$$, where $\\xrightarrow{P}$ denotes convergence in probability. Formally, the continuous mapping theorem states that if:\n",
        "- $X_n\\xrightarrow{P}x$\n",
        "- $f$ is continuous at $x$\n",
        "\n",
        "Then, $f(X_n)\\xrightarrow{P}f(x)$.\n",
        "\n",
        "Rearranging the terms and multiplying by $\\sqrt{n}$ gives\n",
        "\n",
        "$$\\sqrt{n}\\left[g(X_n)-g(\\theta)\\right]=g'(\\tilde\\theta)\\sqrt{n}\\left[X_n-\\theta\\right]$$.\n",
        "\n",
        "Before moving on, Slutsky's theorem says that if:\n",
        "- $A_n\\xrightarrow{D}A$\n",
        "- $B_n\\xrightarrow{P}b$\n",
        "\n",
        "Then, $A_n\\cdot B_n\\xrightarrow{D}A\\cdot b$.\n",
        "\n",
        "Now, since $\\sqrt{n}\\left[X_n-\\theta\\right]\\xrightarrow{D}\\mathcal{N}(0,\\sigma^2)$ by assumption, it follows from appeal to Slutsky's theorem:\n",
        "- $\\sqrt{n}\\left[X_n-\\theta\\right]\\xrightarrow{D}\\mathcal{N}(0,\\sigma^2)$\n",
        "- $g'(\\tilde\\theta)\\xrightarrow{P}g'(\\theta)$\n",
        "\n",
        "Thus, $g'(\\tilde\\theta)\\sqrt{n}\\left[X_n-\\theta\\right]\\xrightarrow{D}g'(\\theta)\\cdot\\mathcal{N}(0,\\sigma^2)$.\n",
        "\n",
        "Finally, using the fact that $K\\cdot\\mathcal{N}(0,\\sigma^2) = \\mathcal{N}(0,K^2\\sigma^2)$ and substitution,\n",
        "\n",
        "$$\\sqrt{n}\\left[g(X_n)-g(\\theta)\\right]\\xrightarrow{D}\\mathcal{N}(0,\\sigma^2\\cdot\\left[g'(\\theta)\\right]^2),$$.\n",
        "\n",
        "$\\text{End of Proof.}$"
      ],
      "metadata": {
        "id": "0TeG4TjmrPiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Delta Method in the Context of AB Testing\n",
        "\n",
        "*Adapted from [Deng and Knoblich (2018)](https://arxiv.org/pdf/1803.06336)*\n",
        "\n",
        "In this section, we'll explore the Delta Method in the context of Online Experimentation at Scale.\n",
        "\n",
        "Deng and Knoblich discuss how in AB Testing, we typically rely on metrics that are traditional \"Average Treatment Effect\" metrics where the estimand neatly obeys the Central Limit Theorem, allowing for quick and intuitive inference via a t-test.\n",
        "\n",
        "For example, the Central Limit Theorem says that if we have $X_1,...,X_n$ iid observations with finite mean $\\mu$ and variance $\\sigma^2 > 0$ and we denote $\\bar X$ as the sample average, then as sample size $n\\rightarrow\\infty$, in distribution\n",
        "\n",
        "$$\\sqrt{n}(\\bar X-\\mu)/\\sigma\\rightarrow N(0,1)$$.\n",
        "\n",
        "A common application of the CLT is to construct the $100(1-\\alpha)\\%$ confidence interval of $\\mu$ as $\\bar X\\pm z_{\\alpha/2}\\sigma/\\sqrt{n}$, where $z_{\\alpha/2}$ is the $(\\alpha/2)$th quantile for $N(0,1)$.\n",
        "\n",
        "Note that in an Average Treatment Effect, it is the difference of 2 means, and the difference of 2 normal distributions is also a normal distribution. Thus, inference on the ATE is straightforward.\n",
        "\n",
        "However, not all metrics of interest are the difference of 2 means, where each mean is sums or averages of iid observations. In such cases, we employ the Delta method, which extends the normal approximations from the CLT more broadly. Abandoning previous mathematical notation to follow this papers, for any random variable $T_n$ (the subscript indicates its dependency on $n$, e.g., sample average) and constant $\\theta$ such that $\\sqrt{n}(T_n-\\theta)\\rightarrow N(0,1)$ in distribution as $n\\rightarrow\\infty$, the Delta method allows us to extend its asymptotic normality to any continuous transformation $\\phi(T_n)$.\n",
        "\n",
        "To be more specific, by using the fact that $T_n-\\theta=O(1/\\sqrt{n})$ and the first order Taylor expansion\n",
        "\n",
        "$$\\phi(T_n)-\\phi(\\theta)=\\phi'(\\theta)(T_n-\\theta)+O\\{(T_n-\\theta)^2\\}$$,\n",
        "\n",
        "we have in distribution\n",
        "\n",
        "$$\\sqrt{n}\\{\\phi(T_n)-\\theta(\\theta)\\}\\rightarrow N\\{0,\\phi'(\\theta)^2\\}$$\n",
        "\n",
        "To elaborate a bit further (as we did not discuss this in the previous theory section), the first order Taylor expansion gives us a remainder term $O(\\cdot)$ that vanishes faster than the Central Limit Theorem of $O(1/\\sqrt{n})$, at a rate of $O(1/n)$. This means that in Asymptotic inference, it is inconsequential, this error will vanish faster than the convergence rate of the Central Limit Theorem and thus it does not matter.\n",
        "\n"
      ],
      "metadata": {
        "id": "WjSkEMttymYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Percent Changes / Ratio Metrics\n",
        "\n",
        "Let $X_1,...,X_n$ be iid observations from the control group with mean $\\mu_x$ and variance $\\sigma_x^2$, and $Y_1,...,Y_n$ iid observations from the treatment group with mean $\\mu_y$ and variance $\\sigma_y^2$, and $\\sigma_{xy}$ the covariance ($0$ in AB tests) between $X_i$'s and $Y_j$s. Let $\\bar X=\\sum_{i=1}^{n}X_i/n$ and $\\bar Y=\\sum_{i=1}^{n}Y_i/n$ be two measurements of the same metric from the treatment and control groups, and their difference $\\hat\\Delta=\\bar Y-\\bar X$ is an unbiased estimate of the ATE $\\Delta=\\mu_y-\\mu_x$. Because both $\\bar X, \\bar Y$ are approximately normally distributed, their difference $\\hat\\Delta$ also follows an approximate normal distribution with mean $\\Delta$ and variance\n",
        "\n",
        "$$\\text{Var}(\\hat\\Delta)=(\\sigma_y^2+\\sigma_x^2-2\\sigma_{xy})/n$$\n",
        "\n",
        "Consequently, the well-known $100(1-\\alpha)\\%$ confidence interval of $\\delta$ is $\\hat\\delta\\pm z_{\\alpha}\\times\\hat{Var}(\\hat\\Delta)$.\n",
        "\n",
        "In practice however, *absolute* differences are hard to interpret because they are not scale-invariant. Thus, there is some interest in focusing on the *relative* difference or percent change $\\Delta\\%=(\\mu_y-\\mu_x)/\\mu_x$, estimated by $\\hat\\Delta\\%=(\\bar Y-\\bar X)/\\bar X$. The key problem is constructing a confidence interval for $\\hat\\Delta\\%$, as we cannot rely on the typical Central Limit Theorem result here, as CLT applies to sums and averages, not ratios of random variables.\n",
        "\n",
        "Traditionally, Fieller (1954) derives a method for computing the standard error here (not shown here), but it is complex and cumbersome to derive. This is where the Delta method comes in."
      ],
      "metadata": {
        "id": "LC10cIWTymdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delta method Derivation for Ratio Metrics\n",
        "\n",
        "The Delta method provides a more intuitive alternative solution.\n",
        "\n",
        "Let's derive the standard error below.\n",
        "\n",
        "Let:\n",
        "- $T_n=(\\bar Y, \\bar X)$\n",
        "- $\\theta=(\\mu_y,\\mu_x)$\n",
        "- $\\phi(x,y)=y/x$\n",
        "\n",
        "The multivariate first order taylor expansion is:\n",
        "\n",
        "$$f(x)\\approx f(a)+\\nabla f(a)^T(x-a)$$\n",
        "\n",
        "(This looks about the same as the univariate version)\n",
        "\n",
        "Thus, when applied to our case, we have:\n",
        "\n",
        "$$\\phi(\\bar Y,\\bar X)\\approx\\phi(\\mu_x,\\mu_y)+\\frac{\\partial\\phi}{\\partial\\bar Y}\\big|_{(\\mu_y,\\mu_x)}(\\bar Y-\\mu_y)+\\frac{\\partial\\phi}{\\partial\\bar X}\\big|_{(\\mu_y,\\mu_x)}(\\bar X-\\mu_x)$$\n",
        "\n",
        "Now we need to derive these partial derivatives of $\\phi=\\bar Y/\\bar X$ w.r.t $\\bar Y$ and $\\bar X$:\n",
        "- w.r.t $\\bar Y\\rightarrow\\frac{\\partial\\phi}{\\partial\\bar Y}=\\frac{1}{\\bar X}$\n",
        "- w.r.t $\\bar X\\rightarrow\\frac{\\partial\\phi}{\\partial\\bar X}=-\\frac{\\bar Y}{\\bar X^2}$\n",
        "\n",
        "Now, evaluate them at $\\theta$ and plug it back into the expression to get:\n",
        "\n",
        "$$\\frac{\\bar Y}{\\bar X}\\approx\\frac{\\mu_y}{\\mu_x}+\\frac{1}{\\mu_x}(\\bar Y-\\mu_y)-\\frac{\\mu_y}{\\mu_x^2}(\\bar X-\\mu_x)$$\n",
        "\n",
        "We can apply the delta method directly by applying $\\nabla g^T\\Sigma\\nabla g$, alternatively, we can compute the variance directly using algebraic rules for playing with variance terms:\n",
        "\n",
        "$$\\text{Var}\\left(\\frac{\\bar Y}{\\bar X}\\right)\\approx\\text{Var}\\left(\\frac{\\mu_y}{\\mu_x}+\\frac{1}{\\mu_x}(\\bar Y-\\mu_y)-\\frac{\\mu_y}{\\mu_x^2}(\\bar X-\\mu_x)\\right)$$\n",
        "\n",
        "After expanding it, we can eliminate a lot of terms because $\\theta=(\\mu_y,\\mu_x)$ has 0 variance since they are fixed. Then, we have:\n",
        "\n",
        "$$\\text{Var}\\left(\\frac{\\bar Y}{\\bar X}\\right)\\approx\\text{Var}\\left(\\frac{1}{\\mu_x}\\bar Y-\\frac{\\mu_y}{\\mu_x^2}\\bar X\\right)$$\n",
        "\n",
        "Recall this rule for the variance of a linear combination:\n",
        "\n",
        "$$\\text{Var}(aU-bV)=a^2\\text{Var}(U)+b^2\\text{Var}(V)-2ab\\text{Cov}(U,V)$$\n",
        "\n",
        "Apply this to our data and we get:\n",
        "\n",
        "$$\\text{Var}\\left(\\frac{\\bar Y}{\\bar X}\\right)\\approx\\frac{1}{\\mu_x^2}\\text{Var}\\left(\\bar Y\\right)-\\left(\\frac{\\mu_y}{\\mu_x^2}\\right)^2\\text{Var}\\left(\\bar X\\right)-2\\frac{1}{\\mu_x}\\frac{\\mu_y}{\\mu_x^2}\\text{Cov}(\\bar Y,\\bar X)$$\n",
        "\n",
        "Now, we substitute:\n",
        "- $\\text{Var}(\\bar Y)=\\sigma_y^2/n$\n",
        "- $\\text{Cov}(\\bar Y,\\bar X)=\\sigma_{xy}/n$\n",
        "\n",
        "To get:\n",
        "\n",
        "$$\\text{Var}\\left(\\frac{\\bar Y}{\\bar X}\\right)\\approx\\frac{1}{\\mu_x^2}\\frac{\\sigma_y^2}{n}+\\left(\\frac{\\mu_y}{\\mu_x^2}\\right)^2\\frac{\\sigma_x^2}{n}-2\\frac{1}{\\mu_x}\\frac{\\mu_y}{\\mu_x^2}\\frac{\\sigma_{xy}}{n}$$\n",
        "\n",
        "Next, we factor out common terms and apply the plug-in principle, replacing the population parameters $\\mu,\\sigma$ with their empirical analogs (sample means and variances). This is justified by Slutsky’s theorem, which ensures that substituting consistent estimators for constants within an expression that converges in distribution preserves that convergence. Since the sample estimates converge in probability to their true values, their inclusion does not affect the limiting distribution.\n",
        "\n",
        "$$\\text{Var}\\left(\\frac{\\bar{Y}}{\\bar{X}}\\right)\n",
        "\\approx \\frac{1}{n\\bar{X}^2} \\left[\n",
        "s_y^2\n",
        "- 2 \\cdot \\frac{\\bar{Y}}{\\bar{X}} \\cdot s_{xy}\n",
        "+ \\left( \\frac{\\bar{Y}}{\\bar{X}} \\right)^2 \\cdot s_x^2\n",
        "\\right]$$\n",
        "\n",
        "That's all! Now, before we show the full confidence interval result, you may be wondering, \"isn't our estimand supposed to be $\\frac{\\bar Y-\\bar X}{\\bar X}$, not $\\frac{\\bar Y}{\\bar X}$? This is because their variances are equivalent. To see this, notice how:\n",
        "\n",
        "$$\\text{Var}\\left(\\frac{\\bar Y-\\bar X}{\\bar X}\\right)=\\text{Var}\\left(\\frac{\\bar Y}{\\bar X}-\\frac{\\bar X}{\\bar X}\\right)=\\text{Var}\\left(\\frac{\\bar Y}{\\bar X}-1\\right)=\\text{Var}\\left(\\frac{\\bar Y}{\\bar X}\\right)$$\n",
        "\n",
        "Thus, our Confidence Interval is:\n",
        "\n",
        "$$\\frac{\\bar Y}{\\bar X}-1\\pm z_{\\alpha/2}\\cdot\\frac{1}{\\sqrt{n}\\bar X}\\sqrt{s_y^2\n",
        "- 2 \\cdot \\frac{\\bar{Y}}{\\bar{X}} \\cdot s_{xy}\n",
        "+ \\left( \\frac{\\bar{Y}}{\\bar{X}} \\right)^2 \\cdot s_x^2\n",
        "}$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gwTK72O6ymha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cluster Randomization\n",
        "\n",
        "Two key concepts in a typical AB test are the *randomization unit* and the *analysis unit*.\n",
        "\n",
        "The Randomization unit refers to the granularity level where the randomization is performed, and the analysis unit is the aggregation level of metric computation.\n",
        "\n",
        "Often, the two are the same, which makes analysis straightforward. However, we can see scenarios where they diverse. Note that the analysis unit must be the same level or *more granular* than randomization, otherwise, the analysis unit may contain observations from both control and treatment. For example, if we randomize by User but our analysis unit is city, that wouldn't make much sense, would it? On the other hand, if we randomize by city, but analyze by user (e.g., average signup rate), we can do it, but not without a standard t-test as \"iid\" would be violated. Once again, the standard CLT assumptions are invalidated.\n",
        "\n",
        "A few more cases is that there is need to reduce bias from network effects, and another is that the same experiment, while randomized by user, might have metrics with different analysis units to meet different business needs. For example, having both user-level and page-level metrics.\n",
        "\n",
        "We consider 2 average metrics of the treatment and control groups, assumed to be independent via randomization. Without loss of generality, we only focus on the Treatment group with $K$ clusters.\n",
        "\n",
        "For $i=1,...,K$, the $i$th cluster contains $N_i$ analysis unit level observations $Y_{ij} (j=1,..,N_i)$.\n",
        "\n",
        "Then, the corresponding average metric is $\\bar Y=\\frac{\\sum_{i,j}Y_{ij}}{\\sum_i N_i}$, which can be interpreted as the total average of all rows of data. Remember, the # of rows correspond to the # of analysis units.\n",
        "\n",
        "We assume that within each cluster $i$, the observations $Y_{ij}$'s are iid with mean $\\mu_i$ and variance $\\sigma_i^2$, and across clusters $(\\mu_i, \\sigma_i,N_i)$ are also iid.\n",
        "\n",
        "To make this easier, think of clusters as Users, and analysis units as page views. We have randomized by users, but we are interested in page-view metrics like \"Page-Load-Time\" or \"Page Crash Rate\".\n",
        "\n",
        "Traditional solutions for calculating variances for $\\text{Var}(\\bar Y)$ exist, dating back to Allan Donner (1987). While not shown here, the formual is incredibly restrictive due to the unrealistic assumptions. For example, it requires equal cluster sizes amongst all users (e.g., # of pages each user loaded is the same) and that the average page load times are the same across all users.\n",
        "\n",
        "Another common approach is the Mixed Effects Model (i.e., multi-level / hierarchical regression, Gelman and Hill, 2006). Under this setting, we can infer the treatment effect as the \"fixed\" effect for the treatment indicator term. Stan (2019) offers a Markov Chain Monte Carlo implementation, which requires significant computational effort for big data. Moreover, the estimate ATE is for the randomization unit but not the analysis unit level. This means that we can craft estimands like \"*difference in average page load time per user between treatment and control*\" but not \"*average treatment effect per pageview across the entire sample*\".\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3-dpKwxk_8Jl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delta Method for Clustered Randomization\n",
        "\n",
        "Recall the following notation:\n",
        "\n",
        "For $i=1,...,K$, the $i$th cluster contains $N_i$ analysis unit level observations $Y_{ij} (j=1,..,N_i)$.\n",
        "\n",
        "Then, the corresponding average metric is $$\\bar Y=\\frac{\\sum_{i,j}Y_{ij}}{\\sum_i N_i}$$.\n",
        "\n",
        "Equivalently, we can rewrite $$\\bar Y=\\frac{\\sum_i(\\sum_jY_{ij})}{\\sum_iN_i}$$\n",
        "\n",
        "Let $S_i=\\sum_iY_{ij}$, and divide both the numerator and denominator by $K$:\n",
        "\n",
        "$$\\bar Y=\\frac{\\sum_iS_i/K}{\\sum_iN_i/K}=\\bar S/\\bar N$$\n",
        "\n",
        "Now, $\\bar Y$ is a ratio of two averages of iid randomization unit level quantities!.\n",
        "\n",
        "Then, we can use the same derivation from before:\n",
        "\n",
        "$$\\text{Var}(\\bar{Y}) \\approx \\frac{1}{K \\mu_N^2} \\left( \\sigma_S^2 - 2 \\frac{\\mu_S}{\\mu_N} \\sigma_{SN} + \\frac{\\mu_S^2}{\\mu_N^2} \\sigma_N^2 \\right)$$\n",
        "\n",
        "There's a few interesting implications here. For example, $\\sigma^2_N=\\text{Var}(N_i)$ implies that there is some desire to make the cluster *sizes* more homogeneous - as if they are a lot of variation, it leads to more variation in the estimate of $\\bar Y$.\n"
      ],
      "metadata": {
        "id": "fJf-LwSC_8Lt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulation\n",
        "\n",
        "First, we'll copy the Data Generation from Deng (2018), which creates a dataset of $K$ rows, one for each cluster (the randomization unit), indexed with $i$.\n",
        "\n",
        "First, we decide if each row is going to be a Small, Medium, or High cluster via a Multinomial Distribution (generalizes the Binomial distribution)\n",
        "\n",
        "$$(M_1,M_2,M_3)'\\sim\\text{Multi-Nomial}\\left(n=K;p=\\left(\\frac{1}{3},\\frac{1}{2},\\frac{1}{6}\\right)\\right)$$\n",
        "\n",
        "Then, once the size for the $i$th row/cluster is decided, we then give them their size and values:\n",
        "- $\\text{Small}:N_i\\sim\\text{Poisson}(2),\\mu_i\\sim\\text{N}(\\mu=0.3,\\sigma=0.05)$\n",
        "- $\\text{Medium}:N_i\\sim\\text{Poisson}(5),\\mu_i\\sim\\text{N}(\\mu=0.5,\\sigma=0.1)$\n",
        "- $\\text{High}:N_i\\sim\\text{Poisson}(30),\\mu_i\\sim\\text{N}(\\mu=0.8,\\sigma=0.05)$\n",
        "\n",
        "Now, for each cluster $i$, we also need output values:\n",
        "\n",
        "$$Y_{ij}\\sim\\text{Bernoulli}(p=\\mu_i)$$ for all $j=1,...,N_i$.\n",
        "\n",
        "To get the \"truth\" value, we compute the weighted average of $\\mu_i$ weighted\n",
        "by the cluster sizes and the mixture of small, medium and large\n",
        "clusters.\n",
        "\n",
        "Let's think about this more intuitively. A naive approach would be to take the average of the 3 $\\mu$'s and then multiply by total # of clusters, $K$, to get the numerator $\\sum_iS_i$. This wouldn't work, because this assumes that the distribution of the different kinds of $\\mu$'s are even.\n",
        "\n",
        "We then instead weigh it by how often that cluster appears, but also multiply how big the clusters are:\n",
        "\n",
        "$$(1/3)*2*0.3+(1/2)*5*0.5+(1/6)*30*0.8$$\n",
        "\n",
        "Now we need the denominator, $\\sum_iN_i$. This is the same as the numerator, but is a count, so you can think of it as all the $\\mu$'s being $1$. Thus,\n",
        "\n",
        "$$(1/3)*2+(1/2)*5+(1/6)*30$$\n",
        "\n",
        "Put it together: this equals $\\approx 0.667$\n",
        "\n",
        "Let's now code this out."
      ],
      "metadata": {
        "id": "Bv41KJlSMuZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "from statsmodels.regression.mixed_linear_model import MixedLM\n",
        "import pandas as pd\n",
        "\n",
        "def delta_variance(N_i, mu_i, sigma_i):\n",
        "  \"\"\"\n",
        "  Computes Variance using Delta Method. i stands for each randomization unit. Should be K rows.\n",
        "  \"\"\"\n",
        "  # num. of clusters, i.e., num. rows\n",
        "  K = len(N_i)\n",
        "\n",
        "  # empirical mean and var. of N_i\n",
        "  mu_N = np.mean(N_i)\n",
        "  sigma_N2 = np.var(N_i, ddof=1)\n",
        "\n",
        "  # empirical mean and var. of S_i\n",
        "  mu_S = np.mean(N_i * mu_i)\n",
        "  sigma_S2 = np.var(N_i * mu_i, ddof=1)\n",
        "\n",
        "  # covariance of s_i and n_i\n",
        "  sigma_SN = np.cov(N_i * mu_i, N_i, ddof=1)[0, 1]\n",
        "\n",
        "  # Delta Variance\n",
        "  var = (1 / (K * mu_N**2)) * (sigma_S2 - 2* (mu_S / mu_N) * sigma_SN + (mu_S**2 / mu_N**2) * sigma_N2)\n",
        "  return var\n",
        "\n",
        "def simulate_run():\n",
        "  # Step 1: Assign Cluster Categories\n",
        "  K = 1000\n",
        "  M = np.random.multinomial(K, [1/3, 1/2, 1/6]) # Small, Medium High\n",
        "  cluster_ids = []\n",
        "  sizes = []\n",
        "  mu_vals = []\n",
        "  sigma_vals = []\n",
        "\n",
        "  # Step 2: Generate N_i, mu_i, sigma_i for each category\n",
        "  categories = [('small', 2, 0.3, 0.05),\n",
        "                ('medium', 5, 0.5, 0.1),\n",
        "                ('high', 30, 0.8, 0.05)]\n",
        "\n",
        "  for m, (label, lam, mu_mean, sigma) in zip(M, categories): # this loop is a 3 x 5 table used as a \"source of truth\" to create the dataset\n",
        "    N_i = np.random.poisson(lam, m)\n",
        "    mu_i = np.random.normal(mu_mean, sigma, m)\n",
        "    mu_i = np.clip(mu_i, 0, 1) # To ensure they are valid Bernoulli\n",
        "    cluster_ids.extend(range(len(cluster_ids), len(cluster_ids) + m))\n",
        "    sizes.extend(N_i)\n",
        "    mu_vals.extend(mu_i)\n",
        "    sigma_vals.extend([sigma]*m) # sigmas are constant across each \"cluster\"-type. So there are 3 different sigmas.\n",
        "\n",
        "  cluster_ids = np.array(cluster_ids)\n",
        "  sizes = np.array(sizes)\n",
        "  mu_vals = np.array(mu_vals)\n",
        "  sigma_vals = np.array(sigma_vals)\n",
        "\n",
        "  # Step 3: Generate observations Y_{ij} ~ Bernoulli(mu_i)\n",
        "  Y_all = [] # after the loop, becomes of size np.sum(sizes)\n",
        "  cluster_label = [] # used to make sure we don't lose track what cluster_id each of the values in Y_all belongs to.\n",
        "  for i in range(len(cluster_ids)):\n",
        "    Y_all.extend(np.random.binomial(1, mu_vals[i], sizes[i]))\n",
        "    cluster_label.extend([i] * sizes[i])\n",
        "\n",
        "  Y_all = np.array(Y_all)\n",
        "  cluster_label = np.array(cluster_label)\n",
        "  total_obs = len(Y_all)\n",
        "\n",
        "  # Step 4: Estimators\n",
        "  naive_mean = np.mean(Y_all)\n",
        "  naive_var_individual = np.var(Y_all, ddof=1) # Variance of individual observations\n",
        "  naive_var_mean = naive_var_individual / total_obs # Naive variance of the mean\n",
        "\n",
        "  # Mixed model\n",
        "  data = pd.DataFrame({\n",
        "      'Y': Y_all,\n",
        "      'cluster': cluster_label\n",
        "  })\n",
        "  try:\n",
        "      model = MixedLM.from_formula(\"Y ~ 1\", groups=\"cluster\", data=data)\n",
        "      result = model.fit(reml=False)\n",
        "      mixed_mean = result.params[\"Intercept\"]\n",
        "      mixed_var = result.bse[\"Intercept\"] ** 2\n",
        "  except:\n",
        "      mixed_mean = np.nan\n",
        "      mixed_var = np.nan\n",
        "\n",
        "  # Delta method\n",
        "  delta_var = delta_variance(sizes, mu_vals, np.array(sigma_vals))\n",
        "  delta_mean = naive_mean\n",
        "\n",
        "  return {\n",
        "      'naive_mean': naive_mean,\n",
        "      'naive_var_mean': naive_var_mean, # Return the naive variance of the mean\n",
        "      'delta_mean': delta_mean,\n",
        "      'delta_var': delta_var,\n",
        "      'mixed_mean': mixed_mean,\n",
        "      'mixed_var': mixed_var,\n",
        "      'total_obs': total_obs # Return total observations for verification\n",
        "  }"
      ],
      "metadata": {
        "id": "bw_tpQKpR0FV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_simulation(n_sim=1000, true_mean=0.667):\n",
        "    results = []\n",
        "    for i in range(n_sim):\n",
        "        results.append(simulate_run())\n",
        "        if i % 100 == 0:\n",
        "            print(f'Simulation {i}/{n_sim} complete')\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    def coverage(mean, var):\n",
        "        lower = mean - 1.96 * np.sqrt(var)\n",
        "        upper = mean + 1.96 * np.sqrt(var)\n",
        "        return (lower <= true_mean) & (upper >= true_mean)\n",
        "\n",
        "    summary = {\n",
        "        'method': [],\n",
        "        'mean_est': [],\n",
        "        'empirical_sd': [],\n",
        "        'avg_est_var': [],\n",
        "        'coverage_95': []\n",
        "    }\n",
        "\n",
        "    for method in ['naive', 'delta', 'mixed']:\n",
        "        means = df[f'{method}_mean']\n",
        "        # Use the calculated variance of the mean for naive method\n",
        "        vars_ = df[f'{method}_var_mean'] if method == 'naive' else df[f'{method}_var']\n",
        "\n",
        "        coverage_vals = coverage(means, vars_)\n",
        "        avg_var = vars_.mean()\n",
        "\n",
        "        summary['method'].append(method)\n",
        "        summary['mean_est'].append(means.mean())\n",
        "        summary['empirical_sd'].append(means.std())\n",
        "        summary['avg_est_var'].append(np.mean(avg_var))\n",
        "        summary['coverage_95'].append(np.mean(coverage_vals))\n",
        "\n",
        "    return pd.DataFrame(summary)"
      ],
      "metadata": {
        "id": "2Bk7zoAhRp14"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_simulation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "KLB4ZCovKSZW",
        "outputId": "c842d9b8-5bd5-4f97-afa4-349835286d9e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulation 0/1000 complete\n",
            "Simulation 100/1000 complete\n",
            "Simulation 200/1000 complete\n",
            "Simulation 300/1000 complete\n",
            "Simulation 400/1000 complete\n",
            "Simulation 500/1000 complete\n",
            "Simulation 600/1000 complete\n",
            "Simulation 700/1000 complete\n",
            "Simulation 800/1000 complete\n",
            "Simulation 900/1000 complete\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  method  mean_est  empirical_sd  avg_est_var  coverage_95\n",
              "0  naive  0.666449      0.009091     0.000027        0.742\n",
              "1  delta  0.666449      0.009091     0.000060        0.905\n",
              "2  mixed  0.545916      0.010064     0.000098        0.000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7b11a353-2b77-4c47-a41d-d95c34140c8f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>method</th>\n",
              "      <th>mean_est</th>\n",
              "      <th>empirical_sd</th>\n",
              "      <th>avg_est_var</th>\n",
              "      <th>coverage_95</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>naive</td>\n",
              "      <td>0.666449</td>\n",
              "      <td>0.009091</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>delta</td>\n",
              "      <td>0.666449</td>\n",
              "      <td>0.009091</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>0.905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mixed</td>\n",
              "      <td>0.545916</td>\n",
              "      <td>0.010064</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7b11a353-2b77-4c47-a41d-d95c34140c8f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7b11a353-2b77-4c47-a41d-d95c34140c8f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7b11a353-2b77-4c47-a41d-d95c34140c8f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ba34725b-6037-48dd-8457-cb055d5fe327\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ba34725b-6037-48dd-8457-cb055d5fe327')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ba34725b-6037-48dd-8457-cb055d5fe327 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"run_simulation()\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"method\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"naive\",\n          \"delta\",\n          \"mixed\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean_est\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06958973433125604,\n        \"min\": 0.5459160932105354,\n        \"max\": 0.666449048757491,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.5459160932105354,\n          0.666449048757491\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"empirical_sd\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0005617231610296414,\n        \"min\": 0.009090946456609945,\n        \"max\": 0.010063879511301478,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.010063879511301478,\n          0.009090946456609945\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_est_var\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.560718442318343e-05,\n        \"min\": 2.7352674087051408e-05,\n        \"max\": 9.849175612071193e-05,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.7352674087051408e-05,\n          6.008707785925332e-05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"coverage_95\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4823826282112572,\n        \"min\": 0.0,\n        \"max\": 0.905,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.742,\n          0.905\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}